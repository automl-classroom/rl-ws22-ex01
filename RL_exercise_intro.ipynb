{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37d9151-dcbf-4c7f-8d0b-c697b3fdbc13",
   "metadata": {},
   "source": [
    "# RL Exercise Demo\n",
    "This exercise serves as a demonstration how to quickly train an RL agent on a popular environment with a RL framework.\n",
    "We will\n",
    "1. Select and instantiate and environment (gym's Hopper-v3).\n",
    "2. Select and setup our RL algorithm / agent (stablebaselines3 TQC [TODO add ref]).\n",
    "3. Train the agent on the environment and visualize training progress.\n",
    "4. Evaluate our agent and observe the distribution of test performances.\n",
    "5. Record and replay the agent, before and after.\n",
    "5. Optimize the agent's hyperparameters with SMAC.\n",
    "\n",
    "Notes\n",
    "- https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4b5e2-e487-4b04-8c55-3938b4d82529",
   "metadata": {},
   "source": [
    "## The Environment: Hopper\n",
    "![Hopper](hopper.gif) [credits](https://www.gymlibrary.dev/_images/hopper.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de499d-f2cd-4d95-b61b-1827650d001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Hopper-v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267d6a0-dcd5-42d2-83e2-a70cf4518e36",
   "metadata": {},
   "source": [
    "## The Agent: TQC\n",
    "[todo add paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b794a7c-86f1-4675-aa8f-9e81f2ca4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stablebaselines3 import TQC\n",
    "model_fn = \"trained_agent.zip\"\n",
    "model = TQC(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "model.save(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c528b3-c890-4d43-9854-6af07bfcc94f",
   "metadata": {},
   "source": [
    "# Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18eb2d7-d711-4b74-bc0f-c01f24658785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stablebaselines3 import TQC\n",
    "\n",
    "env = TrainMonitor(env)  # attach a logger to record video\n",
    "\n",
    "model = TQC.load(model_fn)\n",
    "random_model = TQC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "n_steps = 1000  # run simulation for this number of steps\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(n_steps):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbd131e-67d8-4195-ac93-9b8495d3e780",
   "metadata": {},
   "source": [
    "## Optimize Hyperparameters\n",
    "Often RL algorithms don't converge if the hyperparameters are not configured properly. Or we just want to find out whether we could increase performance further. For this optimize the hyperparameters (HPs) of the RL algorithm/agent.\n",
    "\n",
    "### What Hyperparameters to Optimize?\n",
    "We do not optimize every possible hyperparameter but focus on those which often have a high impact on the learning dynamics, such as the learning rate (step size) or gamma. \n",
    "\n",
    "### How to Set Up the Search Space?\n",
    "For this we need to define our search space, that is, what values we allow for these HPs. It is okay to set a broad range however if there is already an intuition we might set a smaller interval for searching. This way we might get a better solution faster because of the smaller resulting search space. \n",
    "\n",
    "### What Method to Use for Hyperparameter Optimization?\n",
    "Oftentimes people rely on **manual search** for various reasons. However it is often more time-efficient if we use principled and automated methods for the search. Another common approach is **grid search**. This at first seems appealing because we humans like structure. For a HP search space it is better though to sample randomly because we cover the search space way better, see the image below. **So use random search instead of grid search**.\n",
    "[todo add image grid search vs manual search]\n",
    "Those approaches do not use past observed data points to try the next HP configuration. **Bayesian Optimization (BO)** uses a model to incorporate seen data points to approximate the HP optimization landscape. From this model we can smartly sample new HP configurations to try. BO therefore is really sample-efficient (does not need many evaluations/data points to find the optimum. This of course also depends on the number of HPs we would like to optimize).\n",
    "For this optimization we use **SMAC** [todo cite] and especially a multi-fidelity approach (we don't always let the run finish but look early on how the progress and performance is), namely BOHB [todo cite]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f23c1-b24a-442b-846b-a3b3d64acc79",
   "metadata": {},
   "source": [
    "### HPO Naming Conventions\n",
    "- configuration = hyperparameters to use\n",
    "- configuration space = search space, how we can set the hyperparameters\n",
    "- incumbent = best configuration / hyperparameters found so far\n",
    "- target algorithm = the function or algorithm we want to optimize (find the best hyperparameters for)\n",
    "- trajectory = progress of incumbent value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c12263-a256-461b-a2c0-35feb6354f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smac'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msmac\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmac\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMAC4HPO, SMAC4MF, Scenario\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmac\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_space\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigurationSpace, Float, Configuration\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'smac'"
     ]
    }
   ],
   "source": [
    "import smac\n",
    "from smac import SMAC4HPO, SMAC4MF, Scenario\n",
    "from smac.config_space import ConfigurationSpace, Float, Configuration\n",
    "\n",
    "number_of_function_evaluations = 20\n",
    "max_env_steps = 500_000\n",
    "\n",
    "# Build configuration space\n",
    "configuration_space = ConfigurationSpace()\n",
    "gamma = Float(0.9, 0.999, name=\"gamma\", log=True)\n",
    "learning_rate = Float(5e-5, 5e-3, name=\"learning_rate\", log=True)\n",
    "configuration_space.add_hyperparameters([gamma, learning_rate])\n",
    "\n",
    "# In order to use SMAC we need to package the model we want to optimize in a function\n",
    "# In this case, SMAC minimizes therefore we want to return the negative performance\n",
    "def target_algorithm(configuration: Configuration, budget: int) -> float:\n",
    "    env = gym.make(\"Hopper-v3\")\n",
    "    model = TQC(\n",
    "        \"MlpPolicy\", env, verbose=1, gamma=configuration[\"gamma\"], learning_rate=configuration[\"learning_rate\"])\n",
    "    model.learn(total_timesteps=budget)\n",
    "    performance = evaluate(model, env)\n",
    "    return -performance\n",
    "    \n",
    "# Setup SMAC\n",
    "scenario = Scenarion({\n",
    "    \"runcount-limit\": number_of_function_evaluations,\n",
    "    \"eta\": 3,\n",
    "    \n",
    "})\n",
    "smac = SMAC4MF(\n",
    "    scenario=scenario,\n",
    "    target_algorithm=target_algorithm\n",
    ")\n",
    "\n",
    "# Let's go, optimize!\n",
    "smac.optimize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650b521-436b-4176-a059-018b068d92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trajectory\n",
    "smac_outdir = ...\n",
    "trajectory = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
