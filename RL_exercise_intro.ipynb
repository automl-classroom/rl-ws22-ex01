{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37d9151-dcbf-4c7f-8d0b-c697b3fdbc13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RL Exercise Demo\n",
    "This exercise serves as a demonstration how to quickly train an RL agent on a popular environment with a RL framework.\n",
    "We will\n",
    "1. Select and instantiate and environment (gym's BipedalWalker-v3).\n",
    "2. Select and setup our RL algorithm / agent (stablebaselines3 [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html)).\n",
    "3. Train the agent on the environment and visualize training progress.\n",
    "4. Evaluate our agent and observe the distribution of test performances.\n",
    "5. Record and replay the agent, before and after.\n",
    "5. Optimize the agent's hyperparameters with SMAC.\n",
    "\n",
    "Notes\n",
    "- https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4b5e2-e487-4b04-8c55-3938b4d82529",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Environment: Bipedal Walker\n",
    "![Bipedal Walker](bipedal_walker.gif) [credits](https://www.gymlibrary.dev/_images/bipedal_walker.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22de499d-f2cd-4d95-b61b-1827650d001b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_fn = \"trained_agent.zip\"\n",
    "env_id = \"BipedalWalker-v3\"\n",
    "log_dir = \"logs/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267d6a0-dcd5-42d2-83e2-a70cf4518e36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Agent: TQC\n",
    "[todo add paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b794a7c-86f1-4675-aa8f-9e81f2ca4f4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs/tensorboard/SAC_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 458      |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 103      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 1834     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.1    |\n",
      "|    critic_loss     | 0.328    |\n",
      "|    ent_coef        | 0.595    |\n",
      "|    ent_coef_loss   | -3.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1733     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(env_id)\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m SAC(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMlpPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, tensorboard_log\u001B[38;5;241m=\u001B[39mlog_dir)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500_000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39msave(model_fn)\n",
      "File \u001B[0;32m~/.conda/envs/rl01/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:309\u001B[0m, in \u001B[0;36mSAC.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28mself\u001B[39m: SACSelf,\n\u001B[1;32m    297\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    306\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    307\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SACSelf:\n\u001B[0;32m--> 309\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_env\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_log_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_log_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/rl01/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:375\u001B[0m, in \u001B[0;36mOffPolicyAlgorithm.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m         \u001B[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001B[39;00m\n\u001B[1;32m    374\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m gradient_steps \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 375\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgradient_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    377\u001B[0m callback\u001B[38;5;241m.\u001B[39mon_training_end()\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/.conda/envs/rl01/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:277\u001B[0m, in \u001B[0;36mSAC.train\u001B[0;34m(self, gradient_steps, batch_size)\u001B[0m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;66;03m# Optimize the actor\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 277\u001B[0m \u001B[43mactor_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactor\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m    280\u001B[0m \u001B[38;5;66;03m# Update target networks\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/rl01/lib/python3.10/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/rl01/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "env = gym.make(env_id)\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
    "model.learn(total_timesteps=500_000)\n",
    "model.save(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c528b3-c890-4d43-9854-6af07bfcc94f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "video_folder = \"logs/videos/\"\n",
    "video_length = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Record random agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "\n",
    "env = VecVideoRecorder(\n",
    "    env,\n",
    "    video_folder,\n",
    "    record_video_trigger=lambda x: x == 0,\n",
    "    video_length=video_length,\n",
    "    name_prefix=f\"random-agent-{env_id}\",\n",
    ")\n",
    "\n",
    "n_steps = video_length\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(n_steps):\n",
    "    action = [env.action_space.sample()]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        obs = env.reset()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Record trained agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import gym\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "\n",
    "# Record the video starting at the first step\n",
    "env = VecVideoRecorder(\n",
    "    env,\n",
    "    video_folder,\n",
    "    record_video_trigger=lambda x: x == 0,\n",
    "    video_length=video_length,\n",
    "    name_prefix=f\"trained-agent-{env_id}\",\n",
    ")\n",
    "\n",
    "model = SAC.load(model_fn)\n",
    "\n",
    "n_steps = video_length\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(n_steps):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        obs = env.reset()\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimize Hyperparameters\n",
    "Often RL algorithms don't converge if the hyperparameters are not configured properly. Or we just want to find out whether we could increase performance further.\n",
    "For this optimize the hyperparameters (HPs) of the RL algorithm/agent.\n",
    "\n",
    "### What Hyperparameters to Optimize?\n",
    "We do not optimize every possible hyperparameter but focus on those which often have a high impact on the learning dynamics, such as the learning rate (step size) or gamma. \n",
    "\n",
    "### How to Set Up the Search Space?\n",
    "For this we need to define our search space, that is, what values we allow for these HPs.\n",
    "It is okay to set a broad range however if there is already an intuition we might set a smaller interval for searching.\n",
    "This way we might get a better solution faster because of the smaller resulting search space.\n",
    "\n",
    "### What Method to Use for Hyperparameter Optimization?\n",
    "Oftentimes people rely on **manual search** for various reasons. However it is often more time-efficient if we use principled\n",
    "and automated methods for the search. Another common approach is **grid search**.\n",
    "This at first seems appealing because we humans like structure. For a HP search space it is better though to sample randomly\n",
    "because we cover the search space way better, see the image below. **So use random search instead of grid search**.\n",
    "\n",
    "<img src=\"grid_vs_random_search.png\" alt=\"Grid Search vs Random Search\" width=\"800\">\n",
    "\n",
    "[credits](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "Those approaches do not use past observed data points to try the next HP configuration.\n",
    "**Bayesian Optimization (BO)** uses a model to incorporate seen data points to approximate the HP optimization landscape.\n",
    "From this model we can smartly sample new HP configurations to try. BO therefore is really sample-efficient (does not need many evaluations/data points to find the optimum.\n",
    "This of course also depends on the number of HPs we would like to optimize).\n",
    "For this optimization we use [**SMAC**](https://github.com/automl/SMAC3) and especially a multi-fidelity approach (we don't always let the run finish but look early on how the progress and performance is), namely [BOHB](https://proceedings.mlr.press/v80/falkner18a.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HPO Naming Conventions\n",
    "- configuration = hyperparameters to use\n",
    "- configuration space = search space, how we can set the hyperparameters\n",
    "- incumbent = best configuration / hyperparameters found so far\n",
    "- target algorithm = the function or algorithm we want to optimize (find the best hyperparameters for)\n",
    "- trajectory = progress of incumbent value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import gym\n",
    "import numpy as np\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.facade.smac_mf_facade import SMAC4MF\n",
    "from smac.configspace import ConfigurationSpace, UniformFloatHyperparameter, Configuration\n",
    "\n",
    "number_of_function_evaluations = 20\n",
    "max_env_steps = 500_000\n",
    "\n",
    "# Build configuration space\n",
    "configuration_space = ConfigurationSpace()\n",
    "gamma = UniformFloatHyperparameter(lower=0.9, upper=0.999, name=\"gamma\", log=True)\n",
    "learning_rate = UniformFloatHyperparameter(lower=5e-5, upper=5e-3, name=\"learning_rate\", log=True)\n",
    "configuration_space.add_hyperparameters([gamma, learning_rate])\n",
    "\n",
    "# In order to use SMAC we need to package the model we want to optimize in a function\n",
    "# In this case, SMAC minimizes therefore we want to return the negative performance\n",
    "def target_algorithm(configuration: Configuration, budget: int) -> float:\n",
    "    env = gym.make(env_id)\n",
    "    model = SAC(\"MlpPolicy\", env, verbose=0, gamma=configuration[\"gamma\"], learning_rate=configuration[\"learning_rate\"])\n",
    "    model.learn(total_timesteps=budget)\n",
    "    env = Monitor(gym.make(env_id))\n",
    "    means, stds = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "    performance = np.mean(means)\n",
    "    return -performance\n",
    "\n",
    "\n",
    "# Intensifier parameters\n",
    "intensifier_kwargs = {\"initial_budget\": 5000, \"max_budget\": max_env_steps, \"eta\": 3}\n",
    "\n",
    "# Setup SMAC\n",
    "scenario = Scenario(\n",
    "    {\n",
    "        \"run-obj\": \"quality\",  # optimize performance\n",
    "        \"runcount-limit\": number_of_function_evaluations,\n",
    "        \"cs\": configuration_space,\n",
    "        \"output_dir\": \"logs/smac\",\n",
    "    }\n",
    ")\n",
    "smac = SMAC4MF(scenario=scenario, tae_runner=target_algorithm, intensifier_kwargs=intensifier_kwargs, n_jobs=8)\n",
    "\n",
    "# Let's go, optimize!\n",
    "try:\n",
    "    incumbent = smac.optimize()\n",
    "finally:\n",
    "    incumbent = smac.solver.incumbent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650b521-436b-4176-a059-018b068d92ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize trajectory\n",
    "smac_outdir = ...\n",
    "trajectory = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}